{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading all the outputs of load data from load_data function of utils notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing model by passing values in GCN class in models notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(nfeat=features.shape[1], nhidd=20, nclasses=labels.max().item()+1, dropout=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using Adam Optimizer with learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on train dataset using training index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    t = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on test dataset using test index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    \n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.0657 acc_train: 1.0000 loss_val: 0.6257 acc_val: 0.8033 time: 0.0320s\n",
      "Epoch: 0002 loss_train: 0.0653 acc_train: 1.0000 loss_val: 0.6255 acc_val: 0.8033 time: 0.0360s\n",
      "Epoch: 0003 loss_train: 0.0649 acc_train: 1.0000 loss_val: 0.6256 acc_val: 0.8033 time: 0.0240s\n",
      "Epoch: 0004 loss_train: 0.0645 acc_train: 1.0000 loss_val: 0.6262 acc_val: 0.8033 time: 0.0280s\n",
      "Epoch: 0005 loss_train: 0.0642 acc_train: 1.0000 loss_val: 0.6259 acc_val: 0.8033 time: 0.0280s\n",
      "Epoch: 0006 loss_train: 0.0638 acc_train: 1.0000 loss_val: 0.6266 acc_val: 0.8033 time: 0.0280s\n",
      "Epoch: 0007 loss_train: 0.0635 acc_train: 1.0000 loss_val: 0.6268 acc_val: 0.8033 time: 0.0280s\n",
      "Epoch: 0008 loss_train: 0.0632 acc_train: 1.0000 loss_val: 0.6269 acc_val: 0.8033 time: 0.0320s\n",
      "Epoch: 0009 loss_train: 0.0628 acc_train: 1.0000 loss_val: 0.6272 acc_val: 0.8033 time: 0.0280s\n",
      "Epoch: 0010 loss_train: 0.0625 acc_train: 1.0000 loss_val: 0.6280 acc_val: 0.8033 time: 0.0280s\n",
      "Epoch: 0011 loss_train: 0.0622 acc_train: 1.0000 loss_val: 0.6274 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0012 loss_train: 0.0618 acc_train: 1.0000 loss_val: 0.6289 acc_val: 0.8033 time: 0.0240s\n",
      "Epoch: 0013 loss_train: 0.0615 acc_train: 1.0000 loss_val: 0.6280 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0014 loss_train: 0.0612 acc_train: 1.0000 loss_val: 0.6286 acc_val: 0.8000 time: 0.0360s\n",
      "Epoch: 0015 loss_train: 0.0609 acc_train: 1.0000 loss_val: 0.6290 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0016 loss_train: 0.0606 acc_train: 1.0000 loss_val: 0.6294 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0017 loss_train: 0.0603 acc_train: 1.0000 loss_val: 0.6298 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0018 loss_train: 0.0600 acc_train: 1.0000 loss_val: 0.6297 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0019 loss_train: 0.0597 acc_train: 1.0000 loss_val: 0.6305 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0020 loss_train: 0.0595 acc_train: 1.0000 loss_val: 0.6302 acc_val: 0.8000 time: 0.0320s\n",
      "Epoch: 0021 loss_train: 0.0592 acc_train: 1.0000 loss_val: 0.6310 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0022 loss_train: 0.0589 acc_train: 1.0000 loss_val: 0.6310 acc_val: 0.8000 time: 0.0240s\n",
      "Epoch: 0023 loss_train: 0.0586 acc_train: 1.0000 loss_val: 0.6314 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0024 loss_train: 0.0584 acc_train: 1.0000 loss_val: 0.6315 acc_val: 0.8000 time: 0.0240s\n",
      "Epoch: 0025 loss_train: 0.0581 acc_train: 1.0000 loss_val: 0.6320 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0026 loss_train: 0.0578 acc_train: 1.0000 loss_val: 0.6322 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0027 loss_train: 0.0576 acc_train: 1.0000 loss_val: 0.6323 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0028 loss_train: 0.0574 acc_train: 1.0000 loss_val: 0.6333 acc_val: 0.8000 time: 0.0240s\n",
      "Epoch: 0029 loss_train: 0.0571 acc_train: 1.0000 loss_val: 0.6325 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0030 loss_train: 0.0569 acc_train: 1.0000 loss_val: 0.6342 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0031 loss_train: 0.0566 acc_train: 1.0000 loss_val: 0.6331 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0032 loss_train: 0.0564 acc_train: 1.0000 loss_val: 0.6346 acc_val: 0.8000 time: 0.0240s\n",
      "Epoch: 0033 loss_train: 0.0562 acc_train: 1.0000 loss_val: 0.6338 acc_val: 0.8000 time: 0.0320s\n",
      "Epoch: 0034 loss_train: 0.0560 acc_train: 1.0000 loss_val: 0.6347 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0035 loss_train: 0.0557 acc_train: 1.0000 loss_val: 0.6348 acc_val: 0.8000 time: 0.0320s\n",
      "Epoch: 0036 loss_train: 0.0555 acc_train: 1.0000 loss_val: 0.6349 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0037 loss_train: 0.0553 acc_train: 1.0000 loss_val: 0.6361 acc_val: 0.8000 time: 0.0240s\n",
      "Epoch: 0038 loss_train: 0.0551 acc_train: 1.0000 loss_val: 0.6350 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0039 loss_train: 0.0549 acc_train: 1.0000 loss_val: 0.6366 acc_val: 0.8000 time: 0.0320s\n",
      "Epoch: 0040 loss_train: 0.0547 acc_train: 1.0000 loss_val: 0.6359 acc_val: 0.8000 time: 0.0240s\n",
      "Epoch: 0041 loss_train: 0.0545 acc_train: 1.0000 loss_val: 0.6370 acc_val: 0.8000 time: 0.0320s\n",
      "Epoch: 0042 loss_train: 0.0543 acc_train: 1.0000 loss_val: 0.6367 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0043 loss_train: 0.0541 acc_train: 1.0000 loss_val: 0.6373 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0044 loss_train: 0.0539 acc_train: 1.0000 loss_val: 0.6375 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0045 loss_train: 0.0537 acc_train: 1.0000 loss_val: 0.6376 acc_val: 0.8000 time: 0.0320s\n",
      "Epoch: 0046 loss_train: 0.0535 acc_train: 1.0000 loss_val: 0.6389 acc_val: 0.7967 time: 0.0280s\n",
      "Epoch: 0047 loss_train: 0.0533 acc_train: 1.0000 loss_val: 0.6376 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0048 loss_train: 0.0531 acc_train: 1.0000 loss_val: 0.6396 acc_val: 0.7967 time: 0.0240s\n",
      "Epoch: 0049 loss_train: 0.0530 acc_train: 1.0000 loss_val: 0.6381 acc_val: 0.8000 time: 0.0240s\n",
      "Epoch: 0050 loss_train: 0.0528 acc_train: 1.0000 loss_val: 0.6398 acc_val: 0.7967 time: 0.0240s\n",
      "Epoch: 0051 loss_train: 0.0526 acc_train: 1.0000 loss_val: 0.6396 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0052 loss_train: 0.0524 acc_train: 1.0000 loss_val: 0.6395 acc_val: 0.8000 time: 0.0320s\n",
      "Epoch: 0053 loss_train: 0.0522 acc_train: 1.0000 loss_val: 0.6405 acc_val: 0.7967 time: 0.0280s\n",
      "Epoch: 0054 loss_train: 0.0521 acc_train: 1.0000 loss_val: 0.6399 acc_val: 0.8000 time: 0.0240s\n",
      "Epoch: 0055 loss_train: 0.0519 acc_train: 1.0000 loss_val: 0.6411 acc_val: 0.7967 time: 0.0280s\n",
      "Epoch: 0056 loss_train: 0.0517 acc_train: 1.0000 loss_val: 0.6409 acc_val: 0.7967 time: 0.0280s\n",
      "Epoch: 0057 loss_train: 0.0516 acc_train: 1.0000 loss_val: 0.6412 acc_val: 0.7967 time: 0.0320s\n",
      "Epoch: 0058 loss_train: 0.0514 acc_train: 1.0000 loss_val: 0.6419 acc_val: 0.7967 time: 0.0320s\n",
      "Epoch: 0059 loss_train: 0.0513 acc_train: 1.0000 loss_val: 0.6415 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0060 loss_train: 0.0511 acc_train: 1.0000 loss_val: 0.6425 acc_val: 0.7967 time: 0.0280s\n",
      "Epoch: 0061 loss_train: 0.0509 acc_train: 1.0000 loss_val: 0.6424 acc_val: 0.7967 time: 0.0280s\n",
      "Epoch: 0062 loss_train: 0.0508 acc_train: 1.0000 loss_val: 0.6429 acc_val: 0.7967 time: 0.0280s\n",
      "Epoch: 0063 loss_train: 0.0506 acc_train: 1.0000 loss_val: 0.6431 acc_val: 0.7967 time: 0.0280s\n",
      "Epoch: 0064 loss_train: 0.0505 acc_train: 1.0000 loss_val: 0.6435 acc_val: 0.7967 time: 0.0320s\n",
      "Epoch: 0065 loss_train: 0.0503 acc_train: 1.0000 loss_val: 0.6432 acc_val: 0.7967 time: 0.0280s\n",
      "Epoch: 0066 loss_train: 0.0502 acc_train: 1.0000 loss_val: 0.6445 acc_val: 0.7967 time: 0.0240s\n",
      "Epoch: 0067 loss_train: 0.0500 acc_train: 1.0000 loss_val: 0.6430 acc_val: 0.8000 time: 0.0291s\n",
      "Epoch: 0068 loss_train: 0.0499 acc_train: 1.0000 loss_val: 0.6460 acc_val: 0.7967 time: 0.0280s\n",
      "Epoch: 0069 loss_train: 0.0498 acc_train: 1.0000 loss_val: 0.6435 acc_val: 0.8000 time: 0.0202s\n",
      "Epoch: 0070 loss_train: 0.0496 acc_train: 1.0000 loss_val: 0.6459 acc_val: 0.7967 time: 0.0362s\n",
      "Epoch: 0071 loss_train: 0.0495 acc_train: 1.0000 loss_val: 0.6454 acc_val: 0.7967 time: 0.0319s\n",
      "Epoch: 0072 loss_train: 0.0493 acc_train: 1.0000 loss_val: 0.6444 acc_val: 0.8000 time: 0.0280s\n",
      "Epoch: 0073 loss_train: 0.0492 acc_train: 1.0000 loss_val: 0.6469 acc_val: 0.7933 time: 0.0156s\n",
      "Epoch: 0074 loss_train: 0.0491 acc_train: 1.0000 loss_val: 0.6455 acc_val: 0.7967 time: 0.0156s\n",
      "Epoch: 0075 loss_train: 0.0489 acc_train: 1.0000 loss_val: 0.6460 acc_val: 0.7967 time: 0.0312s\n",
      "Epoch: 0076 loss_train: 0.0488 acc_train: 1.0000 loss_val: 0.6474 acc_val: 0.7933 time: 0.0312s\n",
      "Epoch: 0077 loss_train: 0.0487 acc_train: 1.0000 loss_val: 0.6458 acc_val: 0.8000 time: 0.0156s\n",
      "Epoch: 0078 loss_train: 0.0485 acc_train: 1.0000 loss_val: 0.6474 acc_val: 0.7933 time: 0.0283s\n",
      "Epoch: 0079 loss_train: 0.0484 acc_train: 1.0000 loss_val: 0.6476 acc_val: 0.7933 time: 0.0240s\n",
      "Epoch: 0080 loss_train: 0.0483 acc_train: 1.0000 loss_val: 0.6470 acc_val: 0.7933 time: 0.0156s\n",
      "Epoch: 0081 loss_train: 0.0482 acc_train: 1.0000 loss_val: 0.6486 acc_val: 0.7933 time: 0.0313s\n",
      "Epoch: 0082 loss_train: 0.0480 acc_train: 1.0000 loss_val: 0.6476 acc_val: 0.7933 time: 0.0312s\n",
      "Epoch: 0083 loss_train: 0.0479 acc_train: 1.0000 loss_val: 0.6488 acc_val: 0.7933 time: 0.0359s\n",
      "Epoch: 0084 loss_train: 0.0478 acc_train: 1.0000 loss_val: 0.6489 acc_val: 0.7933 time: 0.0164s\n",
      "Epoch: 0085 loss_train: 0.0477 acc_train: 1.0000 loss_val: 0.6486 acc_val: 0.7933 time: 0.0460s\n",
      "Epoch: 0086 loss_train: 0.0475 acc_train: 1.0000 loss_val: 0.6498 acc_val: 0.7933 time: 0.0240s\n",
      "Epoch: 0087 loss_train: 0.0474 acc_train: 1.0000 loss_val: 0.6493 acc_val: 0.7933 time: 0.0237s\n",
      "Epoch: 0088 loss_train: 0.0473 acc_train: 1.0000 loss_val: 0.6498 acc_val: 0.7933 time: 0.0156s\n",
      "Epoch: 0089 loss_train: 0.0472 acc_train: 1.0000 loss_val: 0.6502 acc_val: 0.7933 time: 0.0156s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0090 loss_train: 0.0471 acc_train: 1.0000 loss_val: 0.6496 acc_val: 0.7933 time: 0.0312s\n",
      "Epoch: 0091 loss_train: 0.0469 acc_train: 1.0000 loss_val: 0.6511 acc_val: 0.7900 time: 0.0318s\n",
      "Epoch: 0092 loss_train: 0.0468 acc_train: 1.0000 loss_val: 0.6499 acc_val: 0.7933 time: 0.0280s\n",
      "Epoch: 0093 loss_train: 0.0467 acc_train: 1.0000 loss_val: 0.6515 acc_val: 0.7900 time: 0.0265s\n",
      "Epoch: 0094 loss_train: 0.0466 acc_train: 1.0000 loss_val: 0.6508 acc_val: 0.7933 time: 0.0320s\n",
      "Epoch: 0095 loss_train: 0.0465 acc_train: 1.0000 loss_val: 0.6510 acc_val: 0.7900 time: 0.0360s\n",
      "Epoch: 0096 loss_train: 0.0464 acc_train: 1.0000 loss_val: 0.6524 acc_val: 0.7900 time: 0.0360s\n",
      "Epoch: 0097 loss_train: 0.0463 acc_train: 1.0000 loss_val: 0.6509 acc_val: 0.7900 time: 0.0320s\n",
      "Epoch: 0098 loss_train: 0.0462 acc_train: 1.0000 loss_val: 0.6528 acc_val: 0.7900 time: 0.0400s\n",
      "Epoch: 0099 loss_train: 0.0461 acc_train: 1.0000 loss_val: 0.6523 acc_val: 0.7900 time: 0.0320s\n",
      "Epoch: 0100 loss_train: 0.0460 acc_train: 1.0000 loss_val: 0.6521 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0101 loss_train: 0.0459 acc_train: 1.0000 loss_val: 0.6536 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0102 loss_train: 0.0458 acc_train: 1.0000 loss_val: 0.6525 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0103 loss_train: 0.0457 acc_train: 1.0000 loss_val: 0.6538 acc_val: 0.7900 time: 0.0320s\n",
      "Epoch: 0104 loss_train: 0.0455 acc_train: 1.0000 loss_val: 0.6536 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0105 loss_train: 0.0454 acc_train: 1.0000 loss_val: 0.6536 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0106 loss_train: 0.0453 acc_train: 1.0000 loss_val: 0.6545 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0107 loss_train: 0.0453 acc_train: 1.0000 loss_val: 0.6539 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0108 loss_train: 0.0452 acc_train: 1.0000 loss_val: 0.6548 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0109 loss_train: 0.0451 acc_train: 1.0000 loss_val: 0.6544 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0110 loss_train: 0.0450 acc_train: 1.0000 loss_val: 0.6552 acc_val: 0.7900 time: 0.0320s\n",
      "Epoch: 0111 loss_train: 0.0449 acc_train: 1.0000 loss_val: 0.6550 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0112 loss_train: 0.0448 acc_train: 1.0000 loss_val: 0.6556 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0113 loss_train: 0.0447 acc_train: 1.0000 loss_val: 0.6553 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0114 loss_train: 0.0446 acc_train: 1.0000 loss_val: 0.6563 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0115 loss_train: 0.0445 acc_train: 1.0000 loss_val: 0.6555 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0116 loss_train: 0.0444 acc_train: 1.0000 loss_val: 0.6568 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0117 loss_train: 0.0443 acc_train: 1.0000 loss_val: 0.6561 acc_val: 0.7900 time: 0.0320s\n",
      "Epoch: 0118 loss_train: 0.0442 acc_train: 1.0000 loss_val: 0.6568 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0119 loss_train: 0.0441 acc_train: 1.0000 loss_val: 0.6568 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0120 loss_train: 0.0440 acc_train: 1.0000 loss_val: 0.6566 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0121 loss_train: 0.0439 acc_train: 1.0000 loss_val: 0.6581 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0122 loss_train: 0.0439 acc_train: 1.0000 loss_val: 0.6567 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0123 loss_train: 0.0438 acc_train: 1.0000 loss_val: 0.6587 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0124 loss_train: 0.0437 acc_train: 1.0000 loss_val: 0.6574 acc_val: 0.7900 time: 0.0320s\n",
      "Epoch: 0125 loss_train: 0.0436 acc_train: 1.0000 loss_val: 0.6585 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0126 loss_train: 0.0435 acc_train: 1.0000 loss_val: 0.6587 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0127 loss_train: 0.0434 acc_train: 1.0000 loss_val: 0.6580 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0128 loss_train: 0.0433 acc_train: 1.0000 loss_val: 0.6595 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0129 loss_train: 0.0433 acc_train: 1.0000 loss_val: 0.6584 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0130 loss_train: 0.0432 acc_train: 1.0000 loss_val: 0.6597 acc_val: 0.7900 time: 0.0320s\n",
      "Epoch: 0131 loss_train: 0.0431 acc_train: 1.0000 loss_val: 0.6589 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0132 loss_train: 0.0430 acc_train: 1.0000 loss_val: 0.6599 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0133 loss_train: 0.0429 acc_train: 1.0000 loss_val: 0.6595 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0134 loss_train: 0.0429 acc_train: 1.0000 loss_val: 0.6603 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0135 loss_train: 0.0428 acc_train: 1.0000 loss_val: 0.6595 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0136 loss_train: 0.0427 acc_train: 1.0000 loss_val: 0.6608 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0137 loss_train: 0.0426 acc_train: 1.0000 loss_val: 0.6602 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0138 loss_train: 0.0426 acc_train: 1.0000 loss_val: 0.6609 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0139 loss_train: 0.0425 acc_train: 1.0000 loss_val: 0.6610 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0140 loss_train: 0.0424 acc_train: 1.0000 loss_val: 0.6607 acc_val: 0.7933 time: 0.0280s\n",
      "Epoch: 0141 loss_train: 0.0423 acc_train: 1.0000 loss_val: 0.6621 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0142 loss_train: 0.0422 acc_train: 1.0000 loss_val: 0.6608 acc_val: 0.7933 time: 0.0240s\n",
      "Epoch: 0143 loss_train: 0.0422 acc_train: 1.0000 loss_val: 0.6628 acc_val: 0.7900 time: 0.0320s\n",
      "Epoch: 0144 loss_train: 0.0421 acc_train: 1.0000 loss_val: 0.6611 acc_val: 0.7933 time: 0.0280s\n",
      "Epoch: 0145 loss_train: 0.0420 acc_train: 1.0000 loss_val: 0.6629 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0146 loss_train: 0.0419 acc_train: 1.0000 loss_val: 0.6621 acc_val: 0.7933 time: 0.0240s\n",
      "Epoch: 0147 loss_train: 0.0419 acc_train: 1.0000 loss_val: 0.6623 acc_val: 0.7933 time: 0.0240s\n",
      "Epoch: 0148 loss_train: 0.0418 acc_train: 1.0000 loss_val: 0.6632 acc_val: 0.7867 time: 0.0240s\n",
      "Epoch: 0149 loss_train: 0.0417 acc_train: 1.0000 loss_val: 0.6617 acc_val: 0.7933 time: 0.0240s\n",
      "Epoch: 0150 loss_train: 0.0417 acc_train: 1.0000 loss_val: 0.6640 acc_val: 0.7867 time: 0.0320s\n",
      "Epoch: 0151 loss_train: 0.0416 acc_train: 1.0000 loss_val: 0.6624 acc_val: 0.7933 time: 0.0240s\n",
      "Epoch: 0152 loss_train: 0.0415 acc_train: 1.0000 loss_val: 0.6637 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0153 loss_train: 0.0414 acc_train: 1.0000 loss_val: 0.6640 acc_val: 0.7867 time: 0.0240s\n",
      "Epoch: 0154 loss_train: 0.0414 acc_train: 1.0000 loss_val: 0.6630 acc_val: 0.7933 time: 0.0280s\n",
      "Epoch: 0155 loss_train: 0.0413 acc_train: 1.0000 loss_val: 0.6647 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0156 loss_train: 0.0412 acc_train: 1.0000 loss_val: 0.6635 acc_val: 0.7933 time: 0.0280s\n",
      "Epoch: 0157 loss_train: 0.0412 acc_train: 1.0000 loss_val: 0.6646 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0158 loss_train: 0.0411 acc_train: 1.0000 loss_val: 0.6647 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0159 loss_train: 0.0410 acc_train: 1.0000 loss_val: 0.6644 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0160 loss_train: 0.0410 acc_train: 1.0000 loss_val: 0.6658 acc_val: 0.7867 time: 0.0240s\n",
      "Epoch: 0161 loss_train: 0.0409 acc_train: 1.0000 loss_val: 0.6643 acc_val: 0.7933 time: 0.0240s\n",
      "Epoch: 0162 loss_train: 0.0408 acc_train: 1.0000 loss_val: 0.6659 acc_val: 0.7867 time: 0.0240s\n",
      "Epoch: 0163 loss_train: 0.0408 acc_train: 1.0000 loss_val: 0.6653 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0164 loss_train: 0.0407 acc_train: 1.0000 loss_val: 0.6655 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0165 loss_train: 0.0406 acc_train: 1.0000 loss_val: 0.6661 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0166 loss_train: 0.0406 acc_train: 1.0000 loss_val: 0.6653 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0167 loss_train: 0.0405 acc_train: 1.0000 loss_val: 0.6663 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0168 loss_train: 0.0404 acc_train: 1.0000 loss_val: 0.6658 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0169 loss_train: 0.0404 acc_train: 1.0000 loss_val: 0.6669 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0170 loss_train: 0.0403 acc_train: 1.0000 loss_val: 0.6664 acc_val: 0.7900 time: 0.0320s\n",
      "Epoch: 0171 loss_train: 0.0403 acc_train: 1.0000 loss_val: 0.6669 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0172 loss_train: 0.0402 acc_train: 1.0000 loss_val: 0.6670 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0173 loss_train: 0.0401 acc_train: 1.0000 loss_val: 0.6669 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0174 loss_train: 0.0401 acc_train: 1.0000 loss_val: 0.6677 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0175 loss_train: 0.0400 acc_train: 1.0000 loss_val: 0.6667 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0176 loss_train: 0.0399 acc_train: 1.0000 loss_val: 0.6683 acc_val: 0.7900 time: 0.0320s\n",
      "Epoch: 0177 loss_train: 0.0399 acc_train: 1.0000 loss_val: 0.6670 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0178 loss_train: 0.0398 acc_train: 1.0000 loss_val: 0.6686 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0179 loss_train: 0.0398 acc_train: 1.0000 loss_val: 0.6678 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0180 loss_train: 0.0397 acc_train: 1.0000 loss_val: 0.6684 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0181 loss_train: 0.0396 acc_train: 1.0000 loss_val: 0.6687 acc_val: 0.7900 time: 0.0280s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0182 loss_train: 0.0396 acc_train: 1.0000 loss_val: 0.6682 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0183 loss_train: 0.0395 acc_train: 1.0000 loss_val: 0.6696 acc_val: 0.7867 time: 0.0360s\n",
      "Epoch: 0184 loss_train: 0.0395 acc_train: 1.0000 loss_val: 0.6680 acc_val: 0.7900 time: 0.0320s\n",
      "Epoch: 0185 loss_train: 0.0394 acc_train: 1.0000 loss_val: 0.6701 acc_val: 0.7867 time: 0.0320s\n",
      "Epoch: 0186 loss_train: 0.0394 acc_train: 1.0000 loss_val: 0.6683 acc_val: 0.7900 time: 0.0320s\n",
      "Epoch: 0187 loss_train: 0.0393 acc_train: 1.0000 loss_val: 0.6701 acc_val: 0.7867 time: 0.0320s\n",
      "Epoch: 0188 loss_train: 0.0392 acc_train: 1.0000 loss_val: 0.6690 acc_val: 0.7900 time: 0.0360s\n",
      "Epoch: 0189 loss_train: 0.0392 acc_train: 1.0000 loss_val: 0.6699 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0190 loss_train: 0.0391 acc_train: 1.0000 loss_val: 0.6700 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0191 loss_train: 0.0391 acc_train: 1.0000 loss_val: 0.6695 acc_val: 0.7900 time: 0.0280s\n",
      "Epoch: 0192 loss_train: 0.0390 acc_train: 1.0000 loss_val: 0.6711 acc_val: 0.7867 time: 0.0240s\n",
      "Epoch: 0193 loss_train: 0.0390 acc_train: 1.0000 loss_val: 0.6694 acc_val: 0.7900 time: 0.0240s\n",
      "Epoch: 0194 loss_train: 0.0389 acc_train: 1.0000 loss_val: 0.6713 acc_val: 0.7867 time: 0.0240s\n",
      "Epoch: 0195 loss_train: 0.0389 acc_train: 1.0000 loss_val: 0.6701 acc_val: 0.7900 time: 0.0320s\n",
      "Epoch: 0196 loss_train: 0.0388 acc_train: 1.0000 loss_val: 0.6713 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0197 loss_train: 0.0388 acc_train: 1.0000 loss_val: 0.6707 acc_val: 0.7867 time: 0.0240s\n",
      "Epoch: 0198 loss_train: 0.0387 acc_train: 1.0000 loss_val: 0.6712 acc_val: 0.7867 time: 0.0240s\n",
      "Epoch: 0199 loss_train: 0.0386 acc_train: 1.0000 loss_val: 0.6717 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0200 loss_train: 0.0386 acc_train: 1.0000 loss_val: 0.6711 acc_val: 0.7867 time: 0.0240s\n",
      "Epoch: 0201 loss_train: 0.0385 acc_train: 1.0000 loss_val: 0.6722 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0202 loss_train: 0.0385 acc_train: 1.0000 loss_val: 0.6713 acc_val: 0.7867 time: 0.0320s\n",
      "Epoch: 0203 loss_train: 0.0384 acc_train: 1.0000 loss_val: 0.6723 acc_val: 0.7867 time: 0.0240s\n",
      "Epoch: 0204 loss_train: 0.0384 acc_train: 1.0000 loss_val: 0.6717 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0205 loss_train: 0.0383 acc_train: 1.0000 loss_val: 0.6723 acc_val: 0.7867 time: 0.0240s\n",
      "Epoch: 0206 loss_train: 0.0383 acc_train: 1.0000 loss_val: 0.6724 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0207 loss_train: 0.0382 acc_train: 1.0000 loss_val: 0.6726 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0208 loss_train: 0.0382 acc_train: 1.0000 loss_val: 0.6725 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0209 loss_train: 0.0381 acc_train: 1.0000 loss_val: 0.6729 acc_val: 0.7867 time: 0.0320s\n",
      "Epoch: 0210 loss_train: 0.0381 acc_train: 1.0000 loss_val: 0.6726 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0211 loss_train: 0.0380 acc_train: 1.0000 loss_val: 0.6732 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0212 loss_train: 0.0380 acc_train: 1.0000 loss_val: 0.6728 acc_val: 0.7867 time: 0.0320s\n",
      "Epoch: 0213 loss_train: 0.0379 acc_train: 1.0000 loss_val: 0.6737 acc_val: 0.7867 time: 0.0320s\n",
      "Epoch: 0214 loss_train: 0.0379 acc_train: 1.0000 loss_val: 0.6734 acc_val: 0.7867 time: 0.0320s\n",
      "Epoch: 0215 loss_train: 0.0378 acc_train: 1.0000 loss_val: 0.6739 acc_val: 0.7867 time: 0.0320s\n",
      "Epoch: 0216 loss_train: 0.0378 acc_train: 1.0000 loss_val: 0.6735 acc_val: 0.7867 time: 0.0320s\n",
      "Epoch: 0217 loss_train: 0.0377 acc_train: 1.0000 loss_val: 0.6745 acc_val: 0.7867 time: 0.0320s\n",
      "Epoch: 0218 loss_train: 0.0377 acc_train: 1.0000 loss_val: 0.6736 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0219 loss_train: 0.0377 acc_train: 1.0000 loss_val: 0.6746 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0220 loss_train: 0.0376 acc_train: 1.0000 loss_val: 0.6740 acc_val: 0.7867 time: 0.0320s\n",
      "Epoch: 0221 loss_train: 0.0376 acc_train: 1.0000 loss_val: 0.6748 acc_val: 0.7867 time: 0.0320s\n",
      "Epoch: 0222 loss_train: 0.0375 acc_train: 1.0000 loss_val: 0.6745 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0223 loss_train: 0.0375 acc_train: 1.0000 loss_val: 0.6747 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0224 loss_train: 0.0374 acc_train: 1.0000 loss_val: 0.6748 acc_val: 0.7867 time: 0.0240s\n",
      "Epoch: 0225 loss_train: 0.0374 acc_train: 1.0000 loss_val: 0.6749 acc_val: 0.7867 time: 0.0240s\n",
      "Epoch: 0226 loss_train: 0.0373 acc_train: 1.0000 loss_val: 0.6753 acc_val: 0.7867 time: 0.0240s\n",
      "Epoch: 0227 loss_train: 0.0373 acc_train: 1.0000 loss_val: 0.6747 acc_val: 0.7867 time: 0.0320s\n",
      "Epoch: 0228 loss_train: 0.0372 acc_train: 1.0000 loss_val: 0.6764 acc_val: 0.7867 time: 0.0240s\n",
      "Epoch: 0229 loss_train: 0.0372 acc_train: 1.0000 loss_val: 0.6741 acc_val: 0.7867 time: 0.0240s\n",
      "Epoch: 0230 loss_train: 0.0372 acc_train: 1.0000 loss_val: 0.6774 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0231 loss_train: 0.0371 acc_train: 1.0000 loss_val: 0.6742 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0232 loss_train: 0.0371 acc_train: 1.0000 loss_val: 0.6775 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0233 loss_train: 0.0370 acc_train: 1.0000 loss_val: 0.6752 acc_val: 0.7867 time: 0.0200s\n",
      "Epoch: 0234 loss_train: 0.0370 acc_train: 1.0000 loss_val: 0.6763 acc_val: 0.7867 time: 0.0320s\n",
      "Epoch: 0235 loss_train: 0.0369 acc_train: 1.0000 loss_val: 0.6769 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0236 loss_train: 0.0369 acc_train: 1.0000 loss_val: 0.6754 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0237 loss_train: 0.0368 acc_train: 1.0000 loss_val: 0.6779 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0238 loss_train: 0.0368 acc_train: 1.0000 loss_val: 0.6754 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0239 loss_train: 0.0368 acc_train: 1.0000 loss_val: 0.6779 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0240 loss_train: 0.0367 acc_train: 1.0000 loss_val: 0.6764 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0241 loss_train: 0.0367 acc_train: 1.0000 loss_val: 0.6769 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0242 loss_train: 0.0366 acc_train: 1.0000 loss_val: 0.6776 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0243 loss_train: 0.0366 acc_train: 1.0000 loss_val: 0.6767 acc_val: 0.7867 time: 0.0240s\n",
      "Epoch: 0244 loss_train: 0.0366 acc_train: 1.0000 loss_val: 0.6781 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0245 loss_train: 0.0365 acc_train: 1.0000 loss_val: 0.6772 acc_val: 0.7867 time: 0.0240s\n",
      "Epoch: 0246 loss_train: 0.0365 acc_train: 1.0000 loss_val: 0.6780 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0247 loss_train: 0.0364 acc_train: 1.0000 loss_val: 0.6780 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0248 loss_train: 0.0364 acc_train: 1.0000 loss_val: 0.6779 acc_val: 0.7867 time: 0.0240s\n",
      "Epoch: 0249 loss_train: 0.0363 acc_train: 1.0000 loss_val: 0.6780 acc_val: 0.7867 time: 0.0240s\n",
      "Epoch: 0250 loss_train: 0.0363 acc_train: 1.0000 loss_val: 0.6781 acc_val: 0.7867 time: 0.0240s\n",
      "Epoch: 0251 loss_train: 0.0363 acc_train: 1.0000 loss_val: 0.6784 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0252 loss_train: 0.0362 acc_train: 1.0000 loss_val: 0.6784 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0253 loss_train: 0.0362 acc_train: 1.0000 loss_val: 0.6784 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0254 loss_train: 0.0361 acc_train: 1.0000 loss_val: 0.6786 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0255 loss_train: 0.0361 acc_train: 1.0000 loss_val: 0.6786 acc_val: 0.7867 time: 0.0320s\n",
      "Epoch: 0256 loss_train: 0.0361 acc_train: 1.0000 loss_val: 0.6790 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0257 loss_train: 0.0360 acc_train: 1.0000 loss_val: 0.6788 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0258 loss_train: 0.0360 acc_train: 1.0000 loss_val: 0.6797 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0259 loss_train: 0.0360 acc_train: 1.0000 loss_val: 0.6786 acc_val: 0.7867 time: 0.0280s\n",
      "Epoch: 0260 loss_train: 0.0359 acc_train: 1.0000 loss_val: 0.6800 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0261 loss_train: 0.0359 acc_train: 1.0000 loss_val: 0.6789 acc_val: 0.7867 time: 0.0320s\n",
      "Epoch: 0262 loss_train: 0.0358 acc_train: 1.0000 loss_val: 0.6799 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0263 loss_train: 0.0358 acc_train: 1.0000 loss_val: 0.6796 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0264 loss_train: 0.0358 acc_train: 1.0000 loss_val: 0.6796 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0265 loss_train: 0.0357 acc_train: 1.0000 loss_val: 0.6804 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0266 loss_train: 0.0357 acc_train: 1.0000 loss_val: 0.6794 acc_val: 0.7867 time: 0.0200s\n",
      "Epoch: 0267 loss_train: 0.0357 acc_train: 1.0000 loss_val: 0.6807 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0268 loss_train: 0.0356 acc_train: 1.0000 loss_val: 0.6795 acc_val: 0.7833 time: 0.0320s\n",
      "Epoch: 0269 loss_train: 0.0356 acc_train: 1.0000 loss_val: 0.6809 acc_val: 0.7833 time: 0.0320s\n",
      "Epoch: 0270 loss_train: 0.0355 acc_train: 1.0000 loss_val: 0.6800 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0271 loss_train: 0.0355 acc_train: 1.0000 loss_val: 0.6810 acc_val: 0.7833 time: 0.0320s\n",
      "Epoch: 0272 loss_train: 0.0355 acc_train: 1.0000 loss_val: 0.6806 acc_val: 0.7833 time: 0.0360s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0273 loss_train: 0.0354 acc_train: 1.0000 loss_val: 0.6808 acc_val: 0.7833 time: 0.0320s\n",
      "Epoch: 0274 loss_train: 0.0354 acc_train: 1.0000 loss_val: 0.6811 acc_val: 0.7833 time: 0.0360s\n",
      "Epoch: 0275 loss_train: 0.0354 acc_train: 1.0000 loss_val: 0.6805 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0276 loss_train: 0.0353 acc_train: 1.0000 loss_val: 0.6818 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0277 loss_train: 0.0353 acc_train: 1.0000 loss_val: 0.6803 acc_val: 0.7833 time: 0.0320s\n",
      "Epoch: 0278 loss_train: 0.0353 acc_train: 1.0000 loss_val: 0.6823 acc_val: 0.7833 time: 0.0320s\n",
      "Epoch: 0279 loss_train: 0.0352 acc_train: 1.0000 loss_val: 0.6804 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0280 loss_train: 0.0352 acc_train: 1.0000 loss_val: 0.6822 acc_val: 0.7833 time: 0.0360s\n",
      "Epoch: 0281 loss_train: 0.0351 acc_train: 1.0000 loss_val: 0.6811 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0282 loss_train: 0.0351 acc_train: 1.0000 loss_val: 0.6818 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0283 loss_train: 0.0351 acc_train: 1.0000 loss_val: 0.6818 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0284 loss_train: 0.0351 acc_train: 1.0000 loss_val: 0.6819 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0285 loss_train: 0.0350 acc_train: 1.0000 loss_val: 0.6819 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0286 loss_train: 0.0350 acc_train: 1.0000 loss_val: 0.6823 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0287 loss_train: 0.0349 acc_train: 1.0000 loss_val: 0.6818 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0288 loss_train: 0.0349 acc_train: 1.0000 loss_val: 0.6830 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0289 loss_train: 0.0349 acc_train: 1.0000 loss_val: 0.6818 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0290 loss_train: 0.0348 acc_train: 1.0000 loss_val: 0.6835 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0291 loss_train: 0.0348 acc_train: 1.0000 loss_val: 0.6816 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0292 loss_train: 0.0348 acc_train: 1.0000 loss_val: 0.6837 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0293 loss_train: 0.0348 acc_train: 1.0000 loss_val: 0.6819 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0294 loss_train: 0.0347 acc_train: 1.0000 loss_val: 0.6835 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0295 loss_train: 0.0347 acc_train: 1.0000 loss_val: 0.6825 acc_val: 0.7833 time: 0.0320s\n",
      "Epoch: 0296 loss_train: 0.0347 acc_train: 1.0000 loss_val: 0.6832 acc_val: 0.7833 time: 0.0320s\n",
      "Epoch: 0297 loss_train: 0.0346 acc_train: 1.0000 loss_val: 0.6835 acc_val: 0.7833 time: 0.0320s\n",
      "Epoch: 0298 loss_train: 0.0346 acc_train: 1.0000 loss_val: 0.6824 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0299 loss_train: 0.0346 acc_train: 1.0000 loss_val: 0.6844 acc_val: 0.7833 time: 0.0360s\n",
      "Epoch: 0300 loss_train: 0.0345 acc_train: 1.0000 loss_val: 0.6820 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0301 loss_train: 0.0345 acc_train: 1.0000 loss_val: 0.6848 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0302 loss_train: 0.0345 acc_train: 1.0000 loss_val: 0.6827 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0303 loss_train: 0.0344 acc_train: 1.0000 loss_val: 0.6842 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0304 loss_train: 0.0344 acc_train: 1.0000 loss_val: 0.6836 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0305 loss_train: 0.0344 acc_train: 1.0000 loss_val: 0.6837 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0306 loss_train: 0.0343 acc_train: 1.0000 loss_val: 0.6847 acc_val: 0.7833 time: 0.0320s\n",
      "Epoch: 0307 loss_train: 0.0343 acc_train: 1.0000 loss_val: 0.6833 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0308 loss_train: 0.0343 acc_train: 1.0000 loss_val: 0.6850 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0309 loss_train: 0.0342 acc_train: 1.0000 loss_val: 0.6835 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0310 loss_train: 0.0342 acc_train: 1.0000 loss_val: 0.6848 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0311 loss_train: 0.0342 acc_train: 1.0000 loss_val: 0.6840 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0312 loss_train: 0.0342 acc_train: 1.0000 loss_val: 0.6846 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0313 loss_train: 0.0341 acc_train: 1.0000 loss_val: 0.6848 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0314 loss_train: 0.0341 acc_train: 1.0000 loss_val: 0.6844 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0315 loss_train: 0.0341 acc_train: 1.0000 loss_val: 0.6850 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0316 loss_train: 0.0340 acc_train: 1.0000 loss_val: 0.6844 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0317 loss_train: 0.0340 acc_train: 1.0000 loss_val: 0.6852 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0318 loss_train: 0.0340 acc_train: 1.0000 loss_val: 0.6849 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0319 loss_train: 0.0339 acc_train: 1.0000 loss_val: 0.6850 acc_val: 0.7833 time: 0.0320s\n",
      "Epoch: 0320 loss_train: 0.0339 acc_train: 1.0000 loss_val: 0.6855 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0321 loss_train: 0.0339 acc_train: 1.0000 loss_val: 0.6844 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0322 loss_train: 0.0339 acc_train: 1.0000 loss_val: 0.6865 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0323 loss_train: 0.0338 acc_train: 1.0000 loss_val: 0.6841 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0324 loss_train: 0.0338 acc_train: 1.0000 loss_val: 0.6872 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0325 loss_train: 0.0338 acc_train: 1.0000 loss_val: 0.6842 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0326 loss_train: 0.0338 acc_train: 1.0000 loss_val: 0.6863 acc_val: 0.7833 time: 0.0360s\n",
      "Epoch: 0327 loss_train: 0.0337 acc_train: 1.0000 loss_val: 0.6855 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0328 loss_train: 0.0337 acc_train: 1.0000 loss_val: 0.6854 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0329 loss_train: 0.0337 acc_train: 1.0000 loss_val: 0.6869 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0330 loss_train: 0.0336 acc_train: 1.0000 loss_val: 0.6846 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0331 loss_train: 0.0336 acc_train: 1.0000 loss_val: 0.6875 acc_val: 0.7833 time: 0.0320s\n",
      "Epoch: 0332 loss_train: 0.0336 acc_train: 1.0000 loss_val: 0.6847 acc_val: 0.7833 time: 0.0400s\n",
      "Epoch: 0333 loss_train: 0.0336 acc_train: 1.0000 loss_val: 0.6873 acc_val: 0.7833 time: 0.0360s\n",
      "Epoch: 0334 loss_train: 0.0335 acc_train: 1.0000 loss_val: 0.6859 acc_val: 0.7833 time: 0.0320s\n",
      "Epoch: 0335 loss_train: 0.0335 acc_train: 1.0000 loss_val: 0.6859 acc_val: 0.7833 time: 0.0320s\n",
      "Epoch: 0336 loss_train: 0.0335 acc_train: 1.0000 loss_val: 0.6874 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0337 loss_train: 0.0334 acc_train: 1.0000 loss_val: 0.6853 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0338 loss_train: 0.0334 acc_train: 1.0000 loss_val: 0.6878 acc_val: 0.7833 time: 0.0320s\n",
      "Epoch: 0339 loss_train: 0.0334 acc_train: 1.0000 loss_val: 0.6857 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0340 loss_train: 0.0334 acc_train: 1.0000 loss_val: 0.6872 acc_val: 0.7833 time: 0.0320s\n",
      "Epoch: 0341 loss_train: 0.0333 acc_train: 1.0000 loss_val: 0.6867 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0342 loss_train: 0.0333 acc_train: 1.0000 loss_val: 0.6866 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0343 loss_train: 0.0333 acc_train: 1.0000 loss_val: 0.6873 acc_val: 0.7833 time: 0.0320s\n",
      "Epoch: 0344 loss_train: 0.0333 acc_train: 1.0000 loss_val: 0.6865 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0345 loss_train: 0.0332 acc_train: 1.0000 loss_val: 0.6874 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0346 loss_train: 0.0332 acc_train: 1.0000 loss_val: 0.6870 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0347 loss_train: 0.0332 acc_train: 1.0000 loss_val: 0.6872 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0348 loss_train: 0.0332 acc_train: 1.0000 loss_val: 0.6874 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0349 loss_train: 0.0331 acc_train: 1.0000 loss_val: 0.6869 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0350 loss_train: 0.0331 acc_train: 1.0000 loss_val: 0.6880 acc_val: 0.7833 time: 0.0320s\n",
      "Epoch: 0351 loss_train: 0.0331 acc_train: 1.0000 loss_val: 0.6867 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0352 loss_train: 0.0331 acc_train: 1.0000 loss_val: 0.6883 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0353 loss_train: 0.0330 acc_train: 1.0000 loss_val: 0.6869 acc_val: 0.7833 time: 0.0240s\n",
      "Epoch: 0354 loss_train: 0.0330 acc_train: 1.0000 loss_val: 0.6877 acc_val: 0.7800 time: 0.0240s\n",
      "Epoch: 0355 loss_train: 0.0330 acc_train: 1.0000 loss_val: 0.6878 acc_val: 0.7800 time: 0.0240s\n",
      "Epoch: 0356 loss_train: 0.0330 acc_train: 1.0000 loss_val: 0.6873 acc_val: 0.7833 time: 0.0280s\n",
      "Epoch: 0357 loss_train: 0.0329 acc_train: 1.0000 loss_val: 0.6883 acc_val: 0.7800 time: 0.0280s\n",
      "Epoch: 0358 loss_train: 0.0329 acc_train: 1.0000 loss_val: 0.6873 acc_val: 0.7833 time: 0.0320s\n",
      "Epoch: 0359 loss_train: 0.0329 acc_train: 1.0000 loss_val: 0.6885 acc_val: 0.7800 time: 0.0280s\n",
      "Epoch: 0360 loss_train: 0.0329 acc_train: 1.0000 loss_val: 0.6876 acc_val: 0.7833 time: 0.0200s\n",
      "Epoch: 0361 loss_train: 0.0328 acc_train: 1.0000 loss_val: 0.6887 acc_val: 0.7800 time: 0.0240s\n",
      "Epoch: 0362 loss_train: 0.0328 acc_train: 1.0000 loss_val: 0.6875 acc_val: 0.7833 time: 0.0280s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0363 loss_train: 0.0328 acc_train: 1.0000 loss_val: 0.6886 acc_val: 0.7800 time: 0.0320s\n",
      "Epoch: 0364 loss_train: 0.0328 acc_train: 1.0000 loss_val: 0.6877 acc_val: 0.7800 time: 0.0320s\n",
      "Epoch: 0365 loss_train: 0.0328 acc_train: 1.0000 loss_val: 0.6886 acc_val: 0.7800 time: 0.0320s\n",
      "Epoch: 0366 loss_train: 0.0327 acc_train: 1.0000 loss_val: 0.6883 acc_val: 0.7800 time: 0.0320s\n",
      "Epoch: 0367 loss_train: 0.0327 acc_train: 1.0000 loss_val: 0.6882 acc_val: 0.7800 time: 0.0280s\n",
      "Epoch: 0368 loss_train: 0.0327 acc_train: 1.0000 loss_val: 0.6889 acc_val: 0.7800 time: 0.0320s\n",
      "Epoch: 0369 loss_train: 0.0327 acc_train: 1.0000 loss_val: 0.6880 acc_val: 0.7800 time: 0.0320s\n",
      "Epoch: 0370 loss_train: 0.0326 acc_train: 1.0000 loss_val: 0.6891 acc_val: 0.7800 time: 0.0360s\n",
      "Epoch: 0371 loss_train: 0.0326 acc_train: 1.0000 loss_val: 0.6881 acc_val: 0.7800 time: 0.0320s\n",
      "Epoch: 0372 loss_train: 0.0326 acc_train: 1.0000 loss_val: 0.6892 acc_val: 0.7800 time: 0.0280s\n",
      "Epoch: 0373 loss_train: 0.0326 acc_train: 1.0000 loss_val: 0.6885 acc_val: 0.7800 time: 0.0280s\n",
      "Epoch: 0374 loss_train: 0.0325 acc_train: 1.0000 loss_val: 0.6888 acc_val: 0.7800 time: 0.0280s\n",
      "Epoch: 0375 loss_train: 0.0325 acc_train: 1.0000 loss_val: 0.6891 acc_val: 0.7800 time: 0.0240s\n",
      "Epoch: 0376 loss_train: 0.0325 acc_train: 1.0000 loss_val: 0.6885 acc_val: 0.7800 time: 0.0280s\n",
      "Epoch: 0377 loss_train: 0.0325 acc_train: 1.0000 loss_val: 0.6893 acc_val: 0.7800 time: 0.0280s\n",
      "Epoch: 0378 loss_train: 0.0325 acc_train: 1.0000 loss_val: 0.6885 acc_val: 0.7800 time: 0.0280s\n",
      "Epoch: 0379 loss_train: 0.0324 acc_train: 1.0000 loss_val: 0.6895 acc_val: 0.7800 time: 0.0280s\n",
      "Epoch: 0380 loss_train: 0.0324 acc_train: 1.0000 loss_val: 0.6887 acc_val: 0.7800 time: 0.0280s\n",
      "Epoch: 0381 loss_train: 0.0324 acc_train: 1.0000 loss_val: 0.6895 acc_val: 0.7800 time: 0.0240s\n",
      "Epoch: 0382 loss_train: 0.0324 acc_train: 1.0000 loss_val: 0.6891 acc_val: 0.7800 time: 0.0280s\n",
      "Epoch: 0383 loss_train: 0.0323 acc_train: 1.0000 loss_val: 0.6893 acc_val: 0.7800 time: 0.0320s\n",
      "Epoch: 0384 loss_train: 0.0323 acc_train: 1.0000 loss_val: 0.6895 acc_val: 0.7800 time: 0.0240s\n",
      "Epoch: 0385 loss_train: 0.0323 acc_train: 1.0000 loss_val: 0.6891 acc_val: 0.7800 time: 0.0240s\n",
      "Epoch: 0386 loss_train: 0.0323 acc_train: 1.0000 loss_val: 0.6900 acc_val: 0.7800 time: 0.0280s\n",
      "Epoch: 0387 loss_train: 0.0322 acc_train: 1.0000 loss_val: 0.6888 acc_val: 0.7800 time: 0.0280s\n",
      "Epoch: 0388 loss_train: 0.0322 acc_train: 1.0000 loss_val: 0.6900 acc_val: 0.7800 time: 0.0320s\n",
      "Epoch: 0389 loss_train: 0.0322 acc_train: 1.0000 loss_val: 0.6891 acc_val: 0.7800 time: 0.0360s\n",
      "Epoch: 0390 loss_train: 0.0322 acc_train: 1.0000 loss_val: 0.6898 acc_val: 0.7800 time: 0.0320s\n",
      "Epoch: 0391 loss_train: 0.0322 acc_train: 1.0000 loss_val: 0.6898 acc_val: 0.7800 time: 0.0320s\n",
      "Epoch: 0392 loss_train: 0.0321 acc_train: 1.0000 loss_val: 0.6895 acc_val: 0.7800 time: 0.0320s\n",
      "Epoch: 0393 loss_train: 0.0321 acc_train: 1.0000 loss_val: 0.6902 acc_val: 0.7800 time: 0.0280s\n",
      "Epoch: 0394 loss_train: 0.0321 acc_train: 1.0000 loss_val: 0.6891 acc_val: 0.7800 time: 0.0280s\n",
      "Epoch: 0395 loss_train: 0.0321 acc_train: 1.0000 loss_val: 0.6908 acc_val: 0.7800 time: 0.0400s\n",
      "Epoch: 0396 loss_train: 0.0321 acc_train: 1.0000 loss_val: 0.6889 acc_val: 0.7800 time: 0.0280s\n",
      "Epoch: 0397 loss_train: 0.0320 acc_train: 1.0000 loss_val: 0.6910 acc_val: 0.7800 time: 0.0280s\n",
      "Epoch: 0398 loss_train: 0.0320 acc_train: 1.0000 loss_val: 0.6891 acc_val: 0.7800 time: 0.0320s\n",
      "Epoch: 0399 loss_train: 0.0320 acc_train: 1.0000 loss_val: 0.6909 acc_val: 0.7800 time: 0.0280s\n",
      "Epoch: 0400 loss_train: 0.0320 acc_train: 1.0000 loss_val: 0.6893 acc_val: 0.7800 time: 0.0280s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 52.7837s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(400):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy comes out to be 81.8% on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.5781 accuracy= 0.8180\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
