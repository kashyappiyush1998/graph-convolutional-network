{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers in Graph Convolutional Network\n",
    "\n",
    "In this notebook we will be seeing Graph ConvolutionalNetwork(GCN). GCN are based on structures of a graph, i.e, nodes, edges and weighted edges. But we will be only discussing non-weighted and non-message passing. We will be developing layers using Pytorch. We will input input and output for each layer. We initialize weight and bias as Parameters since in Activation function $y=A(Wx + b)$ W and b are Weight and biases with x as input and y as output. If Bias is given then just declare it as Parameter else you will have to [register_parameter](https://pytorch.org/docs/stable/nn.htmlhighlight=register_parameter#torch.nn.Module.register_parameter).\n",
    "\n",
    "Then we reset the paremeters by standardizing it in range (-stdv,stdv) where stdv is one by square root of number of output features. Here weights.size() outputs the shape of tensor weights and weights.size(1) outputs the shape tuple's dimension 1. Till here our weights and biases are filled and we have number of output and input features. Now we will be forward propagating. \n",
    "\n",
    "For that we will need the input matrix and adjacent matrix. Here adjacent matrix is a sparse matrix which contains 1's where connection between nodes and 0 when there is no connection as the number of nodes increases this sparsity of matrix increases. So, storing mostly zeros is an inefficient way for graphs. For this purpose we have a scipy library for sparse function and here we use torch library for sparse multiplicaton.\n",
    "\n",
    "Sparse matrix is stored in diferent format, i.e, only ones location are stored and rest auomatically are zero. This not just saves storage but also saves search complexity.\n",
    "\n",
    "In Forward Propagation we input the graph matrix or output of previous layer and multiply it by weights and add bias to it so as to form the $Wx+ b$ form but here is a catch. But unlike convoltional neural network which does not considers values farther,i.e, considers or apply function only on what is inside filter our GCN takes all the nodes connected to it in using adjacent matrix. So we do a [sparse matrix multiplication](https://pytorch.org/docs/stable/sparse.html?highlight=spmm#torch.sparse.mm) on our output of $Wx$ matrix. Now we add our bias to it to get our good old form $A(Wx)+b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolutionalNetwork(Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolutionalNetwork, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weights = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.bias = register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0/math.sqrt(self.weights.size(1))\n",
    "        self.weights.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "            \n",
    "    def forward(self, input_, adj):\n",
    "        support = torch.mm(input_, self.weights)\n",
    "        output = torch.sparse.mm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            output+= self.bias\n",
    "            return output\n",
    "        else:\n",
    "            return output\n",
    "        \n",
    "    def __repr__(self):\n",
    "        print(self.__class__.__name__+\"(\" \\\n",
    "              + str(self.in_features)+\"->\" \\\n",
    "              + str(self.out_features))+\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = Parameter(torch.FloatTensor(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
