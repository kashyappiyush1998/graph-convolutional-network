{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "In this notebook we will load our data, normalize the data, one hot encode the classes and find accuracy. All the functions can be called when needed in the training notebook. Our data has 8 classes with 3000 column or papers. Each paper belong to one class and each paper cite at least one other paper. So our one hot encoded matrix wil be of size 3000 X 8 with ones at the place that class the paper it belongs to. Each paper citaction is represented by the edge in adjacent matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./cora/\"\n",
    "dataset = \"cora\"\n",
    "idx_features = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define one hot encoded function - \n",
    "\n",
    "In this we first cconvert all thelabels in classes using set. [Set](https://docs.python.org/3.6/library/stdtypes.html#set) extact all the unique values. This is an iterable data structure. Then we convert our classes in form of dictionary and map them to integers starting from 0-8. You can use this for larger citation datset as well. Then we map all our given labels to the integer that we got from mapping the classes in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoded(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i,:] for i,c in enumerate(classes)}\n",
    "    \n",
    "    one_hot_labels = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    \n",
    "    return one_hot_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "This function is used to load the datset and convert it in form to feed the network. This converts our dataset in and outputs adjacent matrix, features, labels, train data, validation data, test data. But before we start that, I will like you to know one thing that sparse matrix and normal matrix are not possible we have to convert either both of them into [sparse matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html#scipy.sparse.coo_matrix) or covert sparse into dense matrix using [to_dense](https://pytorch.org/docs/stable/sparse.html#torch.sparse.FloatTensor.to_dense) using pytorch.\n",
    "\n",
    "We will use [np.genfromtxt](https://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html) which converts all text data to whatever data type you want to map it into. You can see the dataset, it is in form <paper_id> <word_attributes>+ <class_label>. We will seprate paper_id by selecting $0_th$ index for each paper. Seperate features by selecting $1_st$ to last for each paper and last will represent the class. We will convert class into one hot encoded classes but remember this is not real one hot encoded since it is just in form of integers for labels since the loss function we will be using just accept as integer rather than one hot encoding. we map each paper id to integer 0-n where n is total number of papers.\n",
    "\n",
    "Now we load our cites data using np.genfromtext which contains first data as citing and second as cited paper. We will make our adjacent matrix  out of this eddges dataset. Just for context adjacent matrix is a sparse matrix with 1s at the place where ther is an eddge between nodes also to  be noted that this is a non-directed edge. So it will be symmetric matrix, i.e, a->b then b->a. The step to covert directed edge to undirected edge is a difficult step to understad at first but it is beautifully coded to only keep the largest weight. I would not take credit for explanantion it is explained [here](https://github.com/yao8839836/text_gcn/issues/17). Now we load the data as train index, validation index and test index in our tensor along with features, label and adjacent sparse matrix as adjacent sparse tensor. \n",
    "\n",
    "Allof these tensors are output for training using GCN class we made in models notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path = \"./cora/\", dataset = \"cora\"):\n",
    "    print(\"Loading {} dataset...\".format(dataset))\n",
    "    \n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
    "    idx = np.array(idx_features_labels[:,0], dtype=np.int32)\n",
    "    features = sp.csr_matrix(idx_features_labels[:,1:-1], dtype=np.float32)\n",
    "    labels = one_hot_encoded(idx_features_labels[:,-1])\n",
    "    \n",
    "    idx_map = {j:i for i,j in enumerate(idx)}\n",
    "    \n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten()))).reshape(edges_unordered.shape)\n",
    "    \n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:,0], edges[:,1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T>adj)\n",
    "    \n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj+ sp.eye(adj.shape[0]))\n",
    "    \n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize \n",
    "\n",
    "This normaliization step is more of a hyper paremeter used by writer but has been used differently in other papers differently. This makes our matrix symmetric. Explanation is pretty mathematical.\n",
    "\n",
    "First you have to be introduced to Laplacian matrix. It is Matrix representation of a graph. We normalize the matrix using random walk or making a symmetric normalized laplacian.\n",
    "\n",
    "Here random walk is used-\n",
    "\n",
    "rowsum gives the Degree matrix of each row,i.e, each paper. The you take inverse of each value in degree matrix. Where degree matrix is sum of all the edges for each node,i.e, each paper. Now we set all the values whose degree is 0 to 0 snce there inverse is Infinite. Make a diagonal matrix out of that inverse matrix and multiply it by our given matrix.\n",
    "\n",
    "Formal definition is $L^{rw} = D^{-1}L$, where $D$ is the degree matrix and $L$ is the Laplacian matrix, which is $L = D-A$, where $A$ is the adjancey matrix. Here writer of the paper did not use $L$ but used $A$ directly.\n",
    "\n",
    "More famous is Laplacian symmetric Normalized matrix - \n",
    "\n",
    "$L^{sym} = D^{-\\frac{1}{2}}LD^{\\frac{1}{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(matrix):\n",
    "    rowsum = np.array(matrix.sum(axis=1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    matrix = r_mat_inv.dot(matrix)\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_matrix):\n",
    "    sparse_matrix = sparse_matrix.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_matrix.row, sparse_matrix.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_matrix.data)\n",
    "    shape = torch.Size(sparse_matrix.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
